{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81239f15",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Recursive Self-Aggregation (RSA) - A general-purpose LLM aggregation algorithm using litellm based on the paper **https://rsa-llm.github.io/**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ae2d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9678394d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd7cb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore.all import *\n",
    "from fastcore.test import *\n",
    "from litellm import completion\n",
    "import random, uuid\n",
    "from math import comb\n",
    "from itertools import combinations\n",
    "from fastprogress import progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8def011",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RSACandidate:\n",
    "    \"A candidate response in the RSA algorithm\"\n",
    "    def __init__(self, id:str, loop_id:int, prompt:str, response:str=None, parent_ids:list=None): store_attr()\n",
    "    def __repr__(self): return f'id:{self.id}\\nloop_id:{self.loop_id}\\nprompt:\\n{self.prompt}\\nresponse:\\n{self.response}\\nparent_ids:\\n{self.parent_ids}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fd9e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = RSACandidate(id='c1', loop_id=0, prompt='Hi')\n",
    "c.response = 'Hey'\n",
    "test_eq(c.id, 'c1')\n",
    "test_eq(c.prompt, 'Hi')\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5af69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RSA:\n",
    "    \"Recursive Self-Aggregation algorithm for LLM response aggregation\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        task_prompt:str,  # The main task/question to solve\n",
    "        agg_prompt:str=None,  # Custom aggregation prompt\n",
    "        model:str='openrouter/google/gemini-3-flash-preview',  # LLM model to use\n",
    "        N:int=4,  # Population size (candidates per loop)\n",
    "        K:int=3,  # Number of candidates to aggregate\n",
    "        loops:int=2,  # Number of aggregation loops\n",
    "        history:list=None,  # History of all candidates\n",
    "        temperature:float=1.0,  # LLM temperature\n",
    "        n_workers:int=4  # Parallel workers\n",
    "    ): \n",
    "        if not task_prompt: raise ValueError(\"task_prompt is required\")\n",
    "        if comb(N, K) < N: raise ValueError(f\"C({N},{K})={comb(N,K)} < N={N}; need C(N,K) >= N for aggregation loops\")\n",
    "        store_attr()\n",
    "        if not history: self.history = L()\n",
    "        if not self.agg_prompt: self.agg_prompt = \"\"\"You are given question with training examples and a test input.\\nYou are also provided several candidate solutions. Some candidates may be incorrect\\nAggregate/consider all the candidates and use their help to produce the improved correct solution\"\"\"\n",
    "    \n",
    "    def __repr__(self): return f'RSA(model={self.model!r}, \\nN={self.N}, \\nK={self.K}, \\nloops={self.loops}, \\nhistory={len(self.history)} candidates, \\ntask_prompt={self.task_prompt})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2e65d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = RSA(task_prompt='A bat and ball cost $1.10 total. The bat costs $1 more than the ball. How much does the ball cost?')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c39e9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _call_llm(self:RSA, prompt, **kwargs):\n",
    "    \"Call the LLM with the given prompt and return the response content\"\n",
    "    response = completion(\n",
    "        model=self.model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=self.temperature,\n",
    "        num_retries=3,\n",
    "        **kwargs\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eb32d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "a._call_llm(a.task_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7144ef",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "RSA uses [litellm](https://docs.litellm.ai/) for LLM calls, which automatically reads API keys from environment variables:\n",
    "\n",
    "- `OPENAI_API_KEY` for OpenAI models\n",
    "- `ANTHROPIC_API_KEY` for Anthropic models  \n",
    "- `OPENROUTER_API_KEY` for OpenRouter models\n",
    "- etc.\n",
    "\n",
    "You can also set a custom endpoint globally:\n",
    "\n",
    "```python\n",
    "import litellm\n",
    "litellm.api_base = \"https://your-endpoint.com/v1\"\n",
    "```\n",
    "\n",
    "See [litellm's provider docs](https://docs.litellm.ai/docs/providers) for the full list of supported providers and their environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef493c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _build_agg_prompt(self:RSA, candidates: list[RSACandidate]) -> str:\n",
    "    \"Build an aggregation prompt combining the task prompt with candidate responses\"\n",
    "    parts = [\n",
    "        self.agg_prompt,\n",
    "        self.task_prompt,\n",
    "        \"\\nCANDIDATE ANSWERS (may contain mistakes):\",\n",
    "    ]\n",
    "    for i, cand in enumerate(candidates, 1):\n",
    "        parts.append(f\"---- Candidate {i} ----\\n{cand.response}\")\n",
    "    parts.append(\"\\nYour response:\")\n",
    "    return \"\\n\".join(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bcdce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = RSACandidate(id='c1', loop_id=0, prompt='test', response='Answer A')\n",
    "c2 = RSACandidate(id='c2', loop_id=0, prompt='test', response='Answer B')\n",
    "\n",
    "print(a._build_agg_prompt([c1, c2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81deb527",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_prompts(self:RSA, loop_id, cands=None):\n",
    "    \"Generate candidate prompts for a given loop: N initial candidates, or all C(n,K) combinations for aggregation\"\n",
    "    if not cands: return L(RSACandidate(id=str(uuid.uuid4()), loop_id=loop_id, prompt=self.task_prompt) for _ in range(self.N))\n",
    "    sel_cands = L(combinations(cands, self.K)).shuffle()[:self.N]\n",
    "    return sel_cands.map(lambda x: RSACandidate(id=str(uuid.uuid4()), loop_id=loop_id, prompt=self._build_agg_prompt(x), parent_ids=L(x).attrgot('id')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e394854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loop 0\n",
    "cands = a.get_prompts(loop_id=0)\n",
    "test_eq(len(cands), a.N)\n",
    "test_eq(cands[0].prompt, a.task_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eb53eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loop 1+ (with prior candidates)\n",
    "prior = L(RSACandidate(id=str(uuid.uuid4()), loop_id=0, prompt='test', response=f'Answer {i}') for i in range(8))\n",
    "cands = a.get_prompts(loop_id=1, cands=prior)\n",
    "test_eq(len(cands), a.N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f768643",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cands[0].prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80329d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _run_loop(self:RSA, loop_id, pool=None):\n",
    "    \"Execute one loop: generate prompts, call LLM in parallel, attach responses\"\n",
    "    prompts = self.get_prompts(loop_id, pool)\n",
    "    responses = parallel(self._call_llm, prompts.attrgot('prompt'), n_workers=self.n_workers)\n",
    "    for p, r in zip(prompts, responses): p.response = r\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b9ae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "cands = a._run_loop(loop_id=0)\n",
    "test_eq(len(cands), a.N)\n",
    "assert all(c.response is not None for c in cands)\n",
    "assert cands[0].response != cands[1].response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02599e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def run(self:RSA):\n",
    "    \"Run the full RSA algorithm for the configured number of loops and return the final candidate pool\"\n",
    "    pool = None\n",
    "    pbar = progress_bar(range(self.loops))\n",
    "    for i in pbar:\n",
    "        pbar.comment = f\"Loop {i+1}\"\n",
    "        pool = self._run_loop(i, pool)\n",
    "        self.history.extend(pool)\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd1d6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "a = RSA(task_prompt='A bat and ball cost $1.10 total. The bat costs $1 more than the ball. How much does the ball cost?', loops=2)\n",
    "result = a.run()\n",
    "print(f\"Final pool: {len(result)}, History: {len(a.history)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cb9b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def aggregate(self:RSA, method='llm', final_agg_prompt=None, response_format=None):\n",
    "    \"Final aggregation: one LLM call to aggregate all final loop candidates, with optional structured output\"\n",
    "    if method.lower() not in ['llm', 'random']: raise ValueError(f\"method must be 'llm' or 'random', got {method!r}\")\n",
    "    if not self.history: self.run()\n",
    "    candidates = self.history.filter(lambda x: x.loop_id==(self.loops-1))\n",
    "    if method.lower() == 'random': return '', candidates.shuffle()[0].response\n",
    "    custom_agg_prompt = final_agg_prompt or self.agg_prompt\n",
    "    responses = '\\n'.join(f\"---- Candidate {i+1} ----\\n{c.response}\" for i, c in enumerate(candidates))\n",
    "    prompt = f\"{custom_agg_prompt}\\n\\nPrompt:\\n{self.task_prompt}\\n\\nCANDIDATE ANSWERS:\\n{responses}\\n\\nProvide the best aggregated answer:\"\n",
    "    result = self._call_llm(prompt, **({'response_format': response_format} if response_format else {}))\n",
    "    return prompt, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c318bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "# Test with custom aggregation prompt\n",
    "custom_prompt = \"Combine these answers into one final answer:\"\n",
    "prompt, result = a.aggregate(final_agg_prompt=custom_prompt)\n",
    "assert custom_prompt in prompt\n",
    "assert isinstance(result, str)\n",
    "assert len(result) > 0\n",
    "\n",
    "# Test with response_format (structured output)\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    answer: str\n",
    "    confidence: float\n",
    "\n",
    "prompt, result = a.aggregate(response_format=Answer)\n",
    "assert isinstance(result, str)  # litellm returns JSON string\n",
    "print(prompt, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56276322",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "# Test 'llm' aggregation\n",
    "prompt, result = a.aggregate(method='llm')\n",
    "assert isinstance(prompt, str)\n",
    "assert isinstance(result, str)\n",
    "assert len(result) > 0\n",
    "assert 'CANDIDATE ANSWERS' in prompt\n",
    "\n",
    "# Test 'random' aggregation\n",
    "prompt, result = a.aggregate(method='random')\n",
    "test_eq(prompt, '')\n",
    "assert isinstance(result, str)\n",
    "assert len(result) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9e9a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca5a2ac",
   "metadata": {},
   "source": [
    "how can i improve mine?\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ \n",
    "\n",
    "Here are the key improvements to consider, roughly in order of impact:\n",
    "\n",
    "1. **Different temperatures per phase** â€” Use higher temperature (e.g., 1.0-1.2) for initial generation to maximize diversity, then lower temperature (e.g., 0.7) for aggregation steps where you want more focused synthesis.\n",
    "\n",
    "2. **Tracking diversity** â€” The paper notes diversity should decrease over loops as errors get pruned. You could add a simple diversity metric (e.g., embedding similarity between candidates) to monitor this and potentially adjust parameters.\n",
    "\n",
    "3. **Smarter candidate selection** â€” Instead of random `shuffle()[:N]` for selecting which K-combinations to use, you could prioritize combinations that maximize diversity (picking candidates that are more different from each other).\n",
    "\n",
    "4. **Configurable aggregation prompt per loop** â€” The paper suggests the aggregation task may benefit from different prompting strategies at different stages. You could allow `agg_prompt` to be a callable that takes `loop_id`.\n",
    "\n",
    "5. **Early stopping** â€” If all candidates converge to very similar answers before `loops` is reached, you could stop early.\n",
    "\n",
    "Which of these interests you most? I can help you think through the implementation details."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
